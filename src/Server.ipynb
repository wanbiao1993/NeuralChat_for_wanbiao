{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf127e25-93e5-4fa3-8d8b-ee458e74fde2",
   "metadata": {},
   "source": [
    "# Server端"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b143d4-343c-4d30-820b-8896c3950607",
   "metadata": {},
   "source": [
    "## 1.使用Neural_chat+intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542a968-b078-44bb-8d69-9209c461fef3",
   "metadata": {},
   "source": [
    "```shell\n",
    "su wanbiao # 切换到wanbiao用户，将lib安装到数据盘\n",
    "\n",
    "# 准备环境\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y python3-pip\n",
    "sudo apt-get install -y libgl1-mesa-glx\n",
    "\n",
    "# 准备lib\n",
    "pip install intel-extension-for-transformers\n",
    "pip install fastapi==0.103.2\n",
    "\n",
    "git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "cd intel-extension-for-transformers\n",
    "# 准备依赖\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 下载Phind/Phind-CodeLlama-34B-v2或者codellama/CodeLlama-7b-hf\n",
    "huggingface-cli download --resume-download TheBloke/Phind-CodeLlama-34B-v2\n",
    "# huggingface-cli download --resume-download codellama/CodeLlama-7b-hf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb625f38-35c3-43b1-900e-1069c58c15ab",
   "metadata": {},
   "source": [
    "## 2.修改配置文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77c5bc-dd71-41e6-b363-b270f28d89d1",
   "metadata": {},
   "source": [
    "```yaml\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright (c) 2023 Intel Corporation\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# This is the parameter configuration file for NeuralChat Serving.\n",
    "\n",
    "#################################################################################\n",
    "#                             SERVER SETTING                                    #\n",
    "#################################################################################\n",
    "host: 0.0.0.0\n",
    "port: 6006\n",
    "\n",
    "# model_name_or_path: \"Intel/neural-chat-7b-v3-1\"\n",
    "model_name_or_path: \"codellama/CodeLlama-7b-hf\"\n",
    "# model_name_or_path: \"Phind/Phind-CodeLlama-34B-v2\"\n",
    "# tokenizer_name_or_path: \"\"\n",
    "# peft_model_path: \"\"\n",
    "device: \"auto\"\n",
    "\n",
    "asr:\n",
    "    enable: false\n",
    "    args:\n",
    "        # support cpu, hpu, xpu, cuda\n",
    "        device: \"cpu\"\n",
    "        # support openai/whisper series\n",
    "        model_name_or_path: \"openai/whisper-small\"\n",
    "        # only can be set to true when the device is set to \"cpu\"\n",
    "        bf16: false\n",
    "\n",
    "tts:\n",
    "    enable: false\n",
    "    args:\n",
    "        device: \"cpu\"\n",
    "        voice: \"default\"\n",
    "        stream_mode: false\n",
    "        output_audio_path: \"./output_audio.wav\"\n",
    "\n",
    "tts_multilang:\n",
    "    enable: false\n",
    "    args:\n",
    "        device: \"cpu\"\n",
    "        precision: \"bf16\"\n",
    "\n",
    "retrieval:\n",
    "    enable: false\n",
    "    args:\n",
    "        retrieval_type: \"dense\"\n",
    "        input_path: \"../../assets/docs/\"\n",
    "        embedding_model: \"hkunlp/instructor-large\"\n",
    "        persist_dir: \"./output\"\n",
    "        max_length: 512\n",
    "        process: false\n",
    "\n",
    "cache:\n",
    "    enable: false\n",
    "    args:\n",
    "        config_dir: \"../../pipeline/plugins/caching/cache_config.yaml\"\n",
    "        embedding_model_dir: \"hkunlp/instructor-large\"\n",
    "\n",
    "safety_checker:\n",
    "    enable: false\n",
    "\n",
    "ner:\n",
    "    enable: false\n",
    "    args:\n",
    "        spacy_model: \"en_core_web_lg\"\n",
    "\n",
    "\n",
    "# task choices = ['textchat', 'voicechat', 'retrieval', 'text2image', 'image2image', 'finetune', 'photoai', 'codegen']\n",
    "tasks_list: ['textchat', 'retrieval','codegen']\n",
    "# tasks_list: ['textchat', 'codegen']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cee6d-e2a4-469d-bb7e-925d83d5b6b0",
   "metadata": {},
   "source": [
    "## 3.启动server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a464d-4e82-4e51-852c-e5f4f89a34cc",
   "metadata": {},
   "source": [
    "```shell\n",
    "neuralchat_server start --config_file ./server/config/neuralchat.yaml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
